{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37b85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25285da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "column_names=[\"Comment\", \"Label\", \"Source\"]\n",
    "df_a = pd.read_csv(r'C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\amazon_cells_labelled.txt', names= column_names, sep='\\t', header = None)\n",
    "df_a = df_a.assign(Source='Amazon')\n",
    "df_i = pd.read_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\imdb_labelled.csv\", sep =',', names=column_names, header = None)\n",
    "df_i = df_i.assign(Source='IMDB')\n",
    "df_y = pd.read_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\yelp_labelled.txt\", sep = '\\t', names=column_names, header = None)\n",
    "df_y = df_y.assign(Source = 'Yelp')\n",
    "\n",
    "print(df_a.shape)\n",
    "print(df_i.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49aaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_a, df_i, df_y], ignore_index=True)\n",
    "print(df.shape)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "df.Label = df.Label.astype(int)\n",
    "df.info()\n",
    "sns.countplot(df['Label'], hue =df['Source'], palette ='Set3')\n",
    "\n",
    "#Data is split; 1,500 Positive and 1,500 Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ce5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import advertools as adv\n",
    "emoji_summary = adv.extract_emoji(df)\n",
    "print(emoji_summary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_summary['emoji_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd25996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B-1.A\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all text to lowercase\n",
    "\n",
    "df['comment'] = df['Comment'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all punctuation\n",
    "\n",
    "df['comment'] = df['comment'].str.replace('[^\\w\\s]','')\n",
    "df['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a253d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove emoji's\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df['comment'] = df['comment'].apply(lambda x: remove_emoji(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Kosaka, 2020)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['comment'] = df['comment'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(SpaCy 101: Everything You Need to Know Â· SpaCy Usage Documentation, 2016)\n",
    "#Lemmitization\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "def space(comment):\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "df['comment']= df['comment'].apply(space)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4536b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate vocabulary size\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['comment'])\n",
    "print(\"Vocabulary size: \", len(tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B-1.C\n",
    "#Word Embedding length is the fourth root of the vocabulary size (4445\n",
    "max_sequence_embedding=int(round(np.sqrt(np.sqrt(4445))))\n",
    "print(\"Word embedding length:\") \n",
    "max_sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f230426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B-1.D\n",
    "comment_length = []\n",
    "for char_len in df['comment']:\n",
    "    comment_length.append(len(char_len.split(' ')))\n",
    "    \n",
    "comment_max = np.max(comment_length)\n",
    "comment_min = np.min(comment_length)\n",
    "comment_median = np.median(comment_length)\n",
    "\n",
    "print(\"The maximum length of our sequences would be:\", comment_max)\n",
    "print(\"The minimum length of our sequences would be:\", comment_min)\n",
    "print(\"The median length of our sequences would be:\", comment_median)\n",
    "\n",
    "max_length=41\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop('Comment', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f87c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Source', axis =1)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train/test test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = df['comment']\n",
    "y = df['Label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2 & B3\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#B-1.B\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "train_padded = pad_sequences(train_sequences, padding ='post', maxlen=max_length)\n",
    "test_padded = pad_sequences(test_sequences, padding ='post', maxlen=max_length)\n",
    "\n",
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(y_train)\n",
    "np.asarray(y_test)\n",
    "np.asarray(x_test)\n",
    "np.asarray(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a41b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_training = pd.DataFrame(train_padded)\n",
    "df_x_testing = pd.DataFrame(test_padded)\n",
    "df_y_training = pd.DataFrame(y_train)\n",
    "df_y_testing = pd.DataFrame(y_test)\n",
    "\n",
    "df_x_training.to_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\Padded_x_training.csv\")\n",
    "df_x_testing.to_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\Padded_x_testing.csv\")\n",
    "df_y_training.to_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\Label_train.csv\")\n",
    "df_y_testing.to_csv(r\"C:\\Users\\dbehl\\OneDrive\\WGU\\D213-Task 2\\Label_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "activation = 'sigmoid'\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "#define early_stoopping monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Embedding(vocab_size, max_sequence_embedding, input_length=max_length),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(100, activation=\"relu\"),\n",
    "        layers.Dense(50, activation=\"relu\"),\n",
    "        layers.Dense(1,activation=activation),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33da4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)\n",
    "np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded, y_train, epochs=num_epochs,\n",
    "                  validation_split = 0.3, callbacks=[early_stopping_monitor], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(test_padded, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]}/Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"Accuracy plot.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434172a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D213-Task 2- Sentiment Analysis.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
